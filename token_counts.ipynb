{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "term-statistics.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sirius70/NLP_HW1/blob/main/token_counts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yYCQrU0J5C5"
      },
      "source": [
        "# Empirical Regularities of Language\n",
        "\n",
        "In this first homework assignment, you will familiarize yourself with some empirical regularities of natural language, Shannon entropy and Zipf's Law.\n",
        "\n",
        "Read through this Jupyter notebook and fill in the parts marked with `TODO`. When you're ready to submit, print the notebook as a PDF and upload to Gradescope.\n",
        "\n",
        "## Shannon Entropy\n",
        "\n",
        "Shannon borrowed the concept of entropy from statistical physics to develop _information theory_, focused on encoding and compressing messages. A few years later, in 1950, he applied information theory to analyze human predictive abilityâ€”in other words, the entropy of the human language model. You can read the original article, [Prediction and Entropy of Printed English](https://languagelog.ldc.upenn.edu/myl/Shannon1950.pdf), for more details.\n",
        "\n",
        "Your first task is to collect data on how predictable different letters are in an English sentence, depending on how much context in a word or sentence you have.\n",
        "\n",
        "Go to the [Shannon game page](https://www.ccs.neu.edu/home/dasmith/courses/cs6120/shannon/) that we demonstrated in class. We already guessed part of Text 1, so work through Texts 2, 3, and 4."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Enter the arrays of numbers of guesses for Texts 2, 3, and 4 here.\n",
        "text2_guesses = [13, 6, 5, 3, 1, 1, 1, 1, 9, 2, 2, 1, 1, 2, 1, 1, 2, 1, 11, 3, 4, 2, 1, 1, 1, 1, 1, 8, 4, 3, 2, 1, 10, 1, 1, 1, 1, 6, 5, 5, 1, 1, 1, 1, 1, 1, 1, 0]\n",
        "\n",
        "text3_guesses = [1, 3, 1, 9, 5,\t2, 1, 1, 8, 3, 1, 1, 1,\t1, 1, 1, 1, 1, 1, 1, 3,\t1, 1, 1, 1, 16, 2, 11, 1, 1, 1, 1, 1, 1, 1, 7, 2, 2, 1, 1, 1, 7, 4, 1, 5, 4, 7, 1, 4, 1, 1, 1, 3, 2, 1, 5, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0]\n",
        "\n",
        "text4_guesses = [2, 1, 5, 1, 1,\t8, 3, 2, 2, 3, 1, 1, 1,\t1, 1, 7, 4, 3, 1, 1, 1,\t1, 1, 1, 1, 1, 2, 3, 1, 1, 7, 1, 13, 5, 5, 1, 1, 1, 1, 18, 2, 5, 1, 2, 5, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 0]\n"
      ],
      "metadata": {
        "id": "n6siYs-p0-_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60568059-07ac-41b4-d535-3f62965fa50c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rearrange the guess data into a two-dimensional array, relating number of characters of context (0, 1, 2, ...) to number of guesses required.\n",
        "\n",
        "In other words, you might look in cell (2, 1) and read \"2\" if the number of times it took one guess to get the right letter with two characters of context was 2."
      ],
      "metadata": {
        "id": "3P2UvFmj2Ep2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Create array of counts of guesses. Print out the array so we can see it.\n",
        "import numpy as np\n",
        "\n",
        "def build_table(guesses):\n",
        "    guesses = [g for g in guesses if g > 0]\n",
        "    max_context = len(guesses)\n",
        "    max_guess = max(guesses)\n",
        "    table = np.zeros((max_context, max_guess+1), dtype=int)\n",
        "    for context, guess in enumerate(guesses):\n",
        "        table[context, guess] += 1\n",
        "    return table\n",
        "\n",
        "text2_table = build_table(text2_guesses)\n",
        "text3_table = build_table(text3_guesses)\n",
        "text4_table = build_table(text4_guesses)\n",
        "\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "print(\"Text2 table:\\n\", text2_table)\n",
        "print(\"\\nText3 table:\\n\", text3_table)\n",
        "print(\"\\nText4 table:\\n\", text4_table)\n"
      ],
      "metadata": {
        "id": "deK9pKWC1Qp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "18029cc3-c7de-4dcc-ac00-b8348c6d6da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text2 table:\n",
            " [[0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "\n",
            "Text3 table:\n",
            " [[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "\n",
            "Text4 table:\n",
            " [[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can compute Shannon's upper and lower bounds on the entropy of your predictive distribution for English. The upper bound, as a function of the number of context characters $N$, is just the Shannon entropy of the distribution of numbers of guesses. In other words, it's the entropy of the original text as &ldquo;reduced&rdquo; by the human encoder to a sequence of numbers of guesses.\n",
        "\n",
        "$F_N = -\\sum_{i=1}^{27} q_i^N \\log_2 q_i^N$\n",
        "\n",
        "where $q_i^N$ is the number of times you took $i$ guesses with $N$ characters of context, i.e., one of the cells in the table you computed. The number of guesses ranges from 1 to 27 since we restrict ourselves to 26 letters plus space. In computing entropy, we define $0 \\log 0 = 0$."
      ],
      "metadata": {
        "id": "BR9BqdYO2eTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute the upper bound for each amount of context N and print it out.\n",
        "\n",
        "import math\n",
        "\n",
        "def entropy_upper_bound(guesses):\n",
        "    total = sum(guesses)\n",
        "    H = 0.0\n",
        "    for c in guesses:\n",
        "        if c == 0:\n",
        "            continue\n",
        "        p = c / total\n",
        "        H -= p * math.log2(p)\n",
        "    return H\n",
        "\n",
        "text2_guesses = [13, 6, 5, 3, 1, 1, 1, 1, 9, 2, 2, 1, 1, 2, 1, 1, 2, 1, 11, 3, 4, 2, 1, 1, 1, 1, 1, 8, 4, 3, 2, 1, 10, 1, 1, 1, 1, 6, 5, 5, 1, 1, 1, 1, 1, 1, 1, 0]\n",
        "text3_guesses = [1, 3, 1, 9, 5, 2, 1, 1, 8, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 16, 2, 11, 1, 1, 1, 1, 1, 1, 1, 7, 2, 2, 1, 1, 1, 7, 4, 1, 5, 4, 7, 1, 4, 1, 1, 1, 3, 2, 1, 5, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0]\n",
        "text4_guesses = [2, 1, 5, 1, 1, 8, 3, 2, 2, 3, 1, 1, 1, 1, 1, 7, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 7, 1, 13, 5, 5, 1, 1, 1, 1, 18, 2, 5, 1, 2, 5, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 0]\n",
        "\n",
        "print(\"Upper bound entropies (bits per symbol):\")\n",
        "print(\"Text2:\", entropy_upper_bound(text2_guesses))\n",
        "print(\"Text3:\", entropy_upper_bound(text3_guesses))\n",
        "print(\"Text4:\", entropy_upper_bound(text4_guesses))\n"
      ],
      "metadata": {
        "id": "4ubcFqVv4CjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "288e2e33-bb31-4319-d2fa-f3e6148b6dc5",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upper bound entropies (bits per symbol):\n",
            "Text2: 4.9498598487256\n",
            "Text3: 5.434109439523526\n",
            "Text4: 5.193008890034538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# Define the lists of guesses\n",
        "text2_guesses = [13, 6,\t5, 3, 1, 1, 1, 1, 9, 2, 2, 1, 1, 2, 1, 1, 2, 1, 11, 3, 4, 2, 1, 1, 1, 1, 1, 8, 4, 3, 2, 1, 10, 1, 1, 1, 1, 6, 5, 5, 1, 1, 1, 1, 1, 1, 1]\n",
        "\n",
        "text3_guesses = [1, 3, 1, 9, 5,\t2, 1, 1, 8, 3, 1, 1, 1,\t1, 1, 1, 1, 1, 1, 1, 3,\t1, 1, 1, 1, 16, 2, 11, 1, 1, 1, 1, 1, 1, 1, 7, 2, 2, 1, 1, 1, 7, 4, 1, 5, 4, 7, 1, 4, 1, 1, 1, 3, 2, 1, 5, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n",
        "\n",
        "text4_guesses = [2, 1, 5, 1, 1,\t8, 3, 2, 2, 3, 1, 1, 1,\t1, 1, 7, 4, 3, 1, 1, 1,\t1, 1, 1, 1, 1, 2, 3, 1, 1, 7, 1, 13, 5, 5, 1, 1, 1, 1, 18, 2, 5, 1, 2, 5, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1]\n",
        "\n",
        "\n",
        "# Function to compute the upper bound (Shannon entropy)\n",
        "def compute_entropy(guesses):\n",
        "    # Remove the 0 since we define 0log0 = 0.\n",
        "    filtered_guesses = [g for g in guesses if g != 0]\n",
        "\n",
        "    # Handle the empty list case\n",
        "    if not filtered_guesses:\n",
        "        return 0\n",
        "\n",
        "    # Count the frequencies of each guess number\n",
        "    guess_counts = Counter(filtered_guesses)\n",
        "\n",
        "    # Calculate the total number of guesses\n",
        "    total_guesses = len(filtered_guesses)\n",
        "\n",
        "    entropy = 0.0\n",
        "\n",
        "    # Calculate the entropy using the formula\n",
        "    for count in guess_counts:\n",
        "        print(count)\n",
        "        #print(count.keys(), count.values())\n",
        "        probability = count.values() / total_guesses\n",
        "        print(probability)\n",
        "        # The 0*log(0) part of the sum is defined to be 0.\n",
        "        if probability > 0:\n",
        "            entropy -= probability * math.log2(probability)\n",
        "            print(entropy)\n",
        "\n",
        "    return entropy\n",
        "\n",
        "# Compute the entropy for each text\n",
        "entropy_text2 = compute_entropy(text2_guesses)\n",
        "entropy_text3 = compute_entropy(text3_guesses)\n",
        "entropy_text4 = compute_entropy(text4_guesses)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Upper bound for text2_guesses: {entropy_text2:.2f} bits/character\")\n",
        "print(f\"Upper bound for text3_guesses: {entropy_text3:.2f} bits/character\")\n",
        "print(f\"Upper bound for text4_guesses: {entropy_text4:.2f} bits/character\")"
      ],
      "metadata": {
        "id": "p8d2AabHIYaN",
        "outputId": "e06e7789-4177-436a-f707-8639458a57d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'int' object has no attribute 'values'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-115048835.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Compute the entropy for each text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mentropy_text2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext2_guesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mentropy_text3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext3_guesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mentropy_text4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext4_guesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-115048835.py\u001b[0m in \u001b[0;36mcompute_entropy\u001b[0;34m(guesses)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#print(count.keys(), count.values())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mprobability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_guesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# The 0*log(0) part of the sum is defined to be 0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'values'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shannon derived a lower bound on entropy from the guess data as\n",
        "\n",
        "$\\sum_{i=1}^{27} i(q_i^N - q_{i+1}^N) \\log_2 i$"
      ],
      "metadata": {
        "id": "NRULmLAM4I5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute thew lower bound for each amount of context N and print it out."
      ],
      "metadata": {
        "id": "QjRBkxVdB-4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zipf's Law\n",
        "\n",
        "Now let's look at some text data directly to see the skewed distribution of tokens predicted by Zipf's Law. Recall that Zipf's law states that a word's rank (from the most common word at rank 1 on down) to its frequency is approximately a constant, i.e., $r \\cdot f = k$. Equivalently, we can divide both sides by the total number of tokens $N$ to get $r \\cdot P_r = c$, where $c = k/N$ and $P_r = f/N$ is the _relative frequency_ of word $r$.\n",
        "\n",
        "We start by downloading a sample of 1000 open-access English books from [Project Gutenberg](https://gutenberg.org/)."
      ],
      "metadata": {
        "id": "QRingmXrfJSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If your local environment doesn't have the wget command,\n",
        "# you can comment this out and download it manually.\n",
        "!wget \"http://khoury.northeastern.edu/home/dasmith/pg-sample.json.gz\""
      ],
      "metadata": {
        "id": "iwJS1ClihN_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a123585-c615-4771-ef50-aa6ef4f8f517"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-18 02:38:42--  http://khoury.northeastern.edu/home/dasmith/pg-sample.json.gz\n",
            "Resolving khoury.northeastern.edu (khoury.northeastern.edu)... 52.70.229.197\n",
            "Connecting to khoury.northeastern.edu (khoury.northeastern.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 150984197 (144M) [application/x-gzip]\n",
            "Saving to: â€˜pg-sample.json.gzâ€™\n",
            "\n",
            "pg-sample.json.gz   100%[===================>] 143.99M  81.3MB/s    in 1.8s    \n",
            "\n",
            "2025-09-18 02:38:44 (81.3 MB/s) - â€˜pg-sample.json.gzâ€™ saved [150984197/150984197]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The file is compressed with gzip and is in a JSON lines format. Each line is one JSON record, which we parse with the `json` library.\n",
        "\n",
        "Here we print out the keys in the first record: `id`, `author`, `title`, and `text`."
      ],
      "metadata": {
        "id": "rcOHU_GPiYwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip, json\n",
        "for line in gzip.open(\"pg-sample.json.gz\", mode=\"rt\", encoding=\"utf-8\"):\n",
        "  rec = json.loads(line)\n",
        "  print(rec.keys())\n",
        "  print(rec['author'])\n",
        "  print(rec['title'])\n",
        "  print(rec['text'][0:100])\n",
        "  break\n"
      ],
      "metadata": {
        "id": "_L3x1DRZhfTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task now is to **tokenize** the text in the `text` field of each record into an array of words. Later on in this course, we will discuss learning better tokenizers. For now, you should separate words on whitespace (space, newline, tab) and punctuation. Convert the tokens to lower case, and keep only those tokens that have at least one letter a-z in them. In general, numerals in text tend not to follow Zipf's law but [Benford's law](https://en.wikipedia.org/wiki/Benford%27s_law).\n",
        "\n",
        "You might use _regular expressions_ (e.g., the `re.split` function) to help with tokenization and filtering.\n",
        "\n",
        "After you have tokenized, compute $N$, the total number of tokens in the corpus and print it out."
      ],
      "metadata": {
        "id": "XbfYV9Fyi2UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute an array of tokens in the corpus\n",
        "# Compute the total number of tokens N and print it out.\n",
        "\n",
        "import gzip\n",
        "import json\n",
        "import re\n",
        "\n",
        "all_tokens = []\n",
        "\n",
        "# Precompile regex once (faster than calling inside loop)\n",
        "splitter = re.compile(r'\\W+')\n",
        "\n",
        "with gzip.open(\"pg-sample.json.gz\", mode=\"rt\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        rec = json.loads(line)\n",
        "        text = rec['text']\n",
        "\n",
        "        # Split into tokens\n",
        "        tokens = splitter.split(text)\n",
        "\n",
        "        # Faster check: keep only tokens with at least one letter\n",
        "        tokens = [t.lower() for t in tokens if any(c.isalpha() for c in t)]\n",
        "\n",
        "        all_tokens.extend(tokens)\n",
        "\n",
        "N = len(all_tokens)\n",
        "print(all_tokens)\n",
        "print(\"Total tokens in corpus:\", N)\n"
      ],
      "metadata": {
        "id": "EsVzek9vkgFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, count the frequency each unigram (distinct word) in the corpus and sort them in an array in descending order of frequency. The first item in your array should be the most common word. Print out that word and its frequency"
      ],
      "metadata": {
        "id": "XkRccWKsk4TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute an arrary of unigrams in descending order of frequency.\n",
        "# Print the most common word and its frequency.\n",
        "\n"
      ],
      "metadata": {
        "id": "jhJ7ec3jk2eS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you can look at the Zipf's law relationship between rank and relative frequency (i.e., frequency divided by $N$). Plot the data using a python graphing package such as matplotlib, plotly, or plotnine. This doesn't have to be a fancy graph, so use whatever you're familiar with. Both axes should be on a log scale. If your package doesn't support log scales, you can take the log of the rank and relative frequency yourself before plotting. Recall that since python arrays are zero-indexed, the rank 1 word will be element 0 of your sorted array."
      ],
      "metadata": {
        "id": "ubHNre1ollvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Plot rank vs. relative frequency of unigrams."
      ],
      "metadata": {
        "id": "H1wq5OGBmJP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, take your array of tokens and compute the counts of both the bigrams and trigrams and sort them in descending order of frequency. Print out the most common bigram and trigram."
      ],
      "metadata": {
        "id": "dTVQFNXOmc34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute sorted bigram and trigram statistics.\n",
        "# Print out the most common bigram and trigram.\n",
        "# Plot rank vs. relative frequency for bigrams and trigrams.\n",
        "# You may make separate plots or put them on the same plot and label them."
      ],
      "metadata": {
        "id": "glYoeIsJmm_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: Finally, write your visual impressions of the fit of the unigram, bigram, and trigram distributions. This doesn't need to be statistically rigorous."
      ],
      "metadata": {
        "id": "TdUTKyE6m0qF"
      }
    }
  ]
}